"""
Data loading, cleaning, and feature engineering for disaster ML models.

Handles both real CSVs (EM-DAT, NOAA, FEMA) and the synthetic fallback
generated by scripts/generate_training_data.py.
"""

import logging
from pathlib import Path
from typing import Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

logger = logging.getLogger(__name__)

DATA_DIR = Path(__file__).resolve().parent.parent.parent.parent / "training_data"

# ── Column / category constants ───────────────────────────────────────────

DISASTER_TYPES = [
    "earthquake", "flood", "hurricane", "tornado", "wildfire",
    "tsunami", "drought", "landslide", "volcano",
]

TERRAIN_TYPES = ["flat", "hilly", "mountainous", "forested", "urban", "coastal"]

SEVERITY_ORDER = ["low", "medium", "high", "critical"]


# ── Helpers ───────────────────────────────────────────────────────────────

def _load_csv(name: str) -> pd.DataFrame:
    path = DATA_DIR / name
    if not path.exists():
        raise FileNotFoundError(
            f"{path} not found. Run `python -m scripts.generate_training_data` first."
        )
    return pd.read_csv(path)


def add_rolling_weather_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add engineered weather features — rolling averages and interaction terms."""
    df = df.copy()
    # Wind-humidity interaction (proxy for storm energy)
    df["wind_humidity_idx"] = df["wind_speed"] * df["humidity"] / 100.0
    # Pressure drop from standard atmosphere
    df["pressure_drop"] = 1013.25 - df["pressure"]
    # Temperature deviation from comfortable baseline
    df["temp_deviation"] = (df["temperature"] - 25).abs()
    return df


def encode_disaster_type(df: pd.DataFrame, col: str = "disaster_type") -> pd.DataFrame:
    """One-hot encode disaster type."""
    dummies = pd.get_dummies(df[col], prefix="dtype")
    # Guarantee all expected columns exist
    for dt in DISASTER_TYPES:
        key = f"dtype_{dt}"
        if key not in dummies.columns:
            dummies[key] = 0
    # Deterministic column order
    dummies = dummies[[f"dtype_{dt}" for dt in DISASTER_TYPES]]
    return pd.concat([df.drop(columns=[col]), dummies], axis=1)


def encode_terrain(df: pd.DataFrame, col: str = "terrain_type") -> pd.DataFrame:
    """Ordinal-encode terrain type."""
    mapping = {t: i for i, t in enumerate(TERRAIN_TYPES)}
    df = df.copy()
    df["terrain_idx"] = df[col].map(mapping).fillna(0).astype(int)
    df.drop(columns=[col], inplace=True)
    return df


# ── Public loaders ────────────────────────────────────────────────────────

def load_severity_data(
    test_size: float = 0.2,
    random_state: int = 42,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    Load and prepare the severity classification dataset.

    Returns (X_train, X_test, y_train, y_test).
    Feature columns: temperature, wind_speed, humidity, pressure,
                     one-hot disaster type, engineered features.
    Target: severity label encoded as 0–3.
    """
    df = _load_csv("emdat_severity.csv")

    # Feature engineering
    df = add_rolling_weather_features(df)
    df = encode_disaster_type(df)

    feature_cols = [
        "temperature", "wind_speed", "humidity", "pressure",
        "wind_humidity_idx", "pressure_drop", "temp_deviation",
    ] + [f"dtype_{dt}" for dt in DISASTER_TYPES]

    X = df[feature_cols].astype(float)
    y_raw = df["severity"]

    # Ordinal encode target
    label_map = {s: i for i, s in enumerate(SEVERITY_ORDER)}
    y = y_raw.map(label_map)

    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)


def load_spread_data(
    test_size: float = 0.2,
    random_state: int = 42,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    Load and prepare the spread regression dataset.

    Returns (X_train, X_test, y_train, y_test).
    Target: predicted_area_km2.
    """
    df = _load_csv("spread_area.csv")

    df = encode_disaster_type(df)
    df = encode_terrain(df)

    feature_cols = [
        "current_area_km2", "wind_speed", "wind_direction",
        "elevation_m", "vegetation_density", "days_active", "terrain_idx",
    ] + [f"dtype_{dt}" for dt in DISASTER_TYPES]

    X = df[feature_cols].astype(float)
    y = df["predicted_area_km2"]

    return train_test_split(X, y, test_size=test_size, random_state=random_state)


def load_impact_data(
    test_size: float = 0.2,
    random_state: int = 42,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Load and prepare the multi-output impact dataset.

    Returns (X_train, X_test, y_train, y_test).
    Targets: casualties, economic_damage_million_usd.
    """
    df = _load_csv("impact_casualties.csv")

    df = encode_disaster_type(df)

    feature_cols = [
        "severity_score", "affected_population",
        "gdp_per_capita", "infrastructure_density",
    ] + [f"dtype_{dt}" for dt in DISASTER_TYPES]

    target_cols = ["casualties", "economic_damage_million_usd"]

    X = df[feature_cols].astype(float)
    y = df[target_cols].astype(float)

    return train_test_split(X, y, test_size=test_size, random_state=random_state)
